{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,math\n",
    "from nltk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced cess-esp data\n",
    "\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.tag import hmm\n",
    "\n",
    "\n",
    "s_tagged = nltk.corpus.cess_esp.tagged_sents()\n",
    "\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "for sent in s_tagged:\n",
    "    for word, tag in sent:\n",
    "        n_tag = re.findall(r'^v[a-z]{,2}|^F[a-z]{,2}|^d[a-z]{,1}|^n[a-z]{,1}|^a[a-z]{,1}|^s[a-z]{,1}|^Z[a-z]{,1}|^W[a-z]{,1}|^p[a-z]{,1}', tag)\n",
    "        listToStr = ' '.join(map(str, n_tag)) \n",
    "        list2.append((word,listToStr))\n",
    "    list1.append(list2)\n",
    "    list2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "tagged_sentences = [sentence for sentence in list1]\n",
    "\n",
    "# hold out 20% for testing, get index for 20% split\n",
    "split_idx = math.floor(len(tagged_sentences)*0.2)\n",
    "testing_sentences = tagged_sentences[0:split_idx]\n",
    "training_sentences = tagged_sentences[split_idx:]\n",
    "TEST_EXAMPLE_SENTENCE_INDEX = 676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alberto_Fernández', 'Unk'), (',', 'Fc'), ('que', 'pr'), ('ha', 'vai'), ('presidido', 'aq'), ('la', 'da'), ('primera', 'ao'), ('reunión', 'nc'), ('del', 'sp'), ('grupo', 'nc'), ('del', 'sp'), ('PP', 'np'), ('en', 'sp'), ('el', 'da'), ('Parlament', 'Unk'), ('tras', 'sp'), ('las', 'da'), ('elecciones', 'nc'), (',', 'Fc'), ('ha', 'vai'), ('relativizado', 'Unk'), ('el', 'da'), ('hecho', 'nc'), ('de', 'sp'), ('que', ''), ('CiU', 'np'), ('haya', 'vas'), ('expresado', 'vmp'), ('sus', 'dp'), ('preferencias', 'Unk'), ('por', 'sp'), ('llegar', 'vmn'), ('a', 'sp'), ('acuerdos', 'nc'), ('con', 'sp'), ('ERC', 'np'), ('en', 'sp'), ('Cataluña', 'np'), ('y', ''), ('*0*', 'sn'), ('ha', 'vai'), ('recordado', 'vmp'), ('que', ''), ('la', 'da'), ('coalición', 'nc'), ('también', ''), ('optó', 'vmi'), ('en', 'sp'), ('primer', 'ao'), ('lugar', 'nc'), ('por', 'sp'), ('pactar', 'Unk'), ('con', 'sp'), ('Esquerra', 'Unk'), ('después', ''), ('de', 'sp'), ('los', 'da'), ('comicios', 'nc'), ('autonómicos', 'Unk'), ('y', ''), ('que', ''), (',', 'Fc'), ('al_final', ''), (',', 'Fc'), ('*0*', 'sn'), ('tuvo', 'vmi'), ('que', ''), ('recurrir', 'vmn'), ('a', 'sp'), ('los', 'da'), ('votos', 'nc'), ('del', 'sp'), ('PP', 'np'), ('.', 'Fp')]\n"
     ]
    }
   ],
   "source": [
    "tnt_tagger = nltk.tag.tnt.TnT()\n",
    "tnt_tagger.train(training_sentences)\n",
    "tnt_tagger_preds = tnt_tagger.tag_sents([[word for word,_ in test_sent] for test_sent in testing_sentences])\n",
    "print(tnt_tagger_preds[TEST_EXAMPLE_SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alberto_Fernández', 'np'), (',', 'Fc'), ('que', 'pr'), ('ha', 'vai'), ('presidido', 'vmp'), ('la', 'da'), ('primera', 'ao'), ('reunión', 'nc'), ('del', 'sp'), ('grupo', 'nc'), ('del', 'sp'), ('PP', 'np'), ('en', 'sp'), ('el', 'da'), ('Parlament', 'np'), ('tras', 'sp'), ('las', 'da'), ('elecciones', 'nc'), (',', 'Fc'), ('ha', 'vai'), ('relativizado', 'vmp'), ('el', 'da'), ('hecho', 'nc'), ('de', 'sp'), ('que', ''), ('CiU', 'np'), ('haya', 'vas'), ('expresado', 'vmp'), ('sus', 'dp'), ('preferencias', 'nc'), ('por', 'sp'), ('llegar', 'vmn'), ('a', 'sp'), ('acuerdos', 'nc'), ('con', 'sp'), ('ERC', 'np'), ('en', 'sp'), ('Cataluña', 'np'), ('y', ''), ('*0*', 'sn'), ('ha', 'vai'), ('recordado', 'vmp'), ('que', ''), ('la', 'da'), ('coalición', 'nc'), ('también', ''), ('optó', 'vmi'), ('en', 'sp'), ('primer', 'ao'), ('lugar', 'nc'), ('por', 'sp'), ('pactar', 'vmn'), ('con', 'sp'), ('Esquerra', 'np'), ('después', ''), ('de', 'sp'), ('los', 'da'), ('comicios', 'nc'), ('autonómicos', 'aq'), ('y', ''), ('que', ''), (',', 'Fc'), ('al_final', ''), (',', 'Fc'), ('*0*', 'sn'), ('tuvo', 'vmi'), ('que', ''), ('recurrir', 'vmn'), ('a', 'sp'), ('los', 'da'), ('votos', 'nc'), ('del', 'sp'), ('PP', 'np'), ('.', 'Fp')]\n"
     ]
    }
   ],
   "source": [
    "perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "perceptron_tagger.train(training_sentences)\n",
    "perceptron_tagger_preds = [perceptron_tagger.tag([word for word,_ in test_sentence]) for test_sentence in testing_sentences]\n",
    "print(perceptron_tagger_preds[TEST_EXAMPLE_SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for <nltk.tag.tnt.TnT object at 0x12a556910>\n",
      " Accuracy : 0.88492413549753\n",
      " Precision: 0.9834545909492033\n",
      " Recall   : 0.88492413549753\n",
      " F1-Score : 0.9230045897013717\n",
      "\n",
      "Metrics for <nltk.tag.perceptron.PerceptronTagger object at 0x12a5566a0>\n",
      " Accuracy : 0.9735797459421313\n",
      " Precision: 0.9733551859619565\n",
      " Recall   : 0.9735797459421313\n",
      " F1-Score : 0.972926017548243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "golds = [tag for sentence in testing_sentences for _,tag in sentence]\n",
    "tnt_tagger_pred_labels = [tag for sentence in tnt_tagger_preds for _,tag in sentence]\n",
    "perceptron_tagger_pred_labels = [tag for sentence in perceptron_tagger_preds for _,tag in sentence]\n",
    "\n",
    "for preds,tagger in [(tnt_tagger_pred_labels,tnt_tagger),(perceptron_tagger_pred_labels,perceptron_tagger)]:\n",
    "    print(\"Metrics for\",tagger)\n",
    "    print(\" Accuracy :\", metrics.accuracy_score(golds,preds))\n",
    "    print(\" Precision:\", metrics.precision_score(golds,preds,average='weighted'))\n",
    "    print(\" Recall   :\", metrics.recall_score(golds,preds,average='weighted'))\n",
    "    print(\" F1-Score :\", metrics.f1_score(golds,preds,average='weighted'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import brill, brill_trainer \n",
    "\n",
    "def train_brill_tagger(initial_tagger, train_sents, **kwargs): \n",
    "    templates = [ \n",
    "            brill.Template(brill.Pos([-1])), \n",
    "            brill.Template(brill.Pos([1])), \n",
    "            brill.Template(brill.Pos([-2])), \n",
    "            brill.Template(brill.Pos([2])), \n",
    "            brill.Template(brill.Pos([-2, -1])), \n",
    "            brill.Template(brill.Pos([1, 2])), \n",
    "            brill.Template(brill.Pos([-3, -2, -1])), \n",
    "            brill.Template(brill.Pos([1, 2, 3])), \n",
    "            brill.Template(brill.Pos([-1]), brill.Pos([1])), \n",
    "            brill.Template(brill.Word([-1])), \n",
    "            brill.Template(brill.Word([1])), \n",
    "            brill.Template(brill.Word([-2])), \n",
    "            brill.Template(brill.Word([2])), \n",
    "            brill.Template(brill.Word([-2, -1])), \n",
    "            brill.Template(brill.Word([1, 2])), \n",
    "            brill.Template(brill.Word([-3, -2, -1])), \n",
    "            brill.Template(brill.Word([1, 2, 3])), \n",
    "            brill.Template(brill.Word([-1]), brill.Word([1])), \n",
    "            ] \n",
    "      \n",
    "    # Using BrillTaggerTrainer to train  \n",
    "    trainer = brill_trainer.BrillTaggerTrainer( \n",
    "            initial_tagger, templates, deterministic = True) \n",
    "      \n",
    "    return trainer.train(train_sents, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Back-off Initial Tag :  0.8809324276640791\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import brill, brill_trainer \n",
    "from nltk.tag import DefaultTagger \n",
    "import itertools\n",
    "from nltk.tbl import Template\n",
    "from nltk.tag import brill, brill_trainer\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "\tfor cls in tagger_classes:\n",
    "\t\tbackoff = cls(train_sents, backoff=backoff)\n",
    "\t\n",
    "\treturn backoff\n",
    "\n",
    "def word_tag_model(words, tagged_words, limit=200):\n",
    "\tfd = FreqDist(words)\n",
    "\tcfd = ConditionalFreqDist(tagged_words)\n",
    "\tmost_freq = (word for word, count in fd.most_common(limit))\n",
    "\treturn dict((word, cfd[word].max()) for word in most_freq)\n",
    "\n",
    "patterns = [\n",
    "\t(r'^\\d+$', 'CD'),\n",
    "\t(r'.*ing$', 'VBG'), # gerunds, i.e. wondering\n",
    "\t(r'.*ment$', 'NN'), # i.e. wonderment\n",
    "\t(r'.*ful$', 'JJ') # i.e. wonderful\n",
    "]\n",
    "\n",
    "def unigram_feature_detector(tokens, index, history):\n",
    "\treturn {'word': tokens[index]}\n",
    "  \n",
    "# Initializing \n",
    "default_tagger = DefaultTagger('NN') \n",
    "  \n",
    "initial_tag = backoff_tagger( \n",
    "        training_sentences, [UnigramTagger, BigramTagger,  \n",
    "                    TrigramTagger], backoff = default_tagger) \n",
    "      \n",
    "a = initial_tag.evaluate(testing_sentences) \n",
    "print (\"Accuracy of Back-off Initial Tag : \", a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of brill_tag :  0.883424488355681\n"
     ]
    }
   ],
   "source": [
    "brill_tag = train_brill_tagger(initial_tag, training_sentences) \n",
    "b = brill_tag.evaluate(testing_sentences) \n",
    "  \n",
    "print (\"Accuracy of brill_tag : \", b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Hmm Initial Tag :  0.9204745942131263\n"
     ]
    }
   ],
   "source": [
    "initial_tag2 = nltk.HiddenMarkovModelTagger.train(training_sentences)\n",
    "\n",
    "a = initial_tag2.evaluate(testing_sentences) \n",
    "print (\"Accuracy of Hmm Initial Tag : \", a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of brill_tag :  0.9225696894848271\n"
     ]
    }
   ],
   "source": [
    "brill_tag = train_brill_tagger(initial_tag2, training_sentences) \n",
    "b = brill_tag.evaluate(testing_sentences) \n",
    "  \n",
    "print (\"Accuracy of brill_tag : \", b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
